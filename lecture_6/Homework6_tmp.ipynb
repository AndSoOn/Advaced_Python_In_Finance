{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Set 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "D. Bindel and J. Goodman: Principles of Scientific Computing, Chapter 6, Exercise 7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "#### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\phi(t)$ is minimized when $\\phi^{'}(t) = 0$  \n",
    "i.e.  \n",
    "$$\\begin{array}{l}\n",
    "\\phi^{'}(t) &= \\nabla V(\\bar{x} + tp_{k})^{T} p_{k}\\\\\n",
    "&= p_{k+1}^{T}p_{k}\\\\\n",
    "&= 0\n",
    "\\end{array}$$\n",
    "Which shows $p_{k+1}$ and $p_{k}$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\nabla V(\\bar{x}) = (\\lambda_{1}x_{1}, \\lambda_{2}x_{2})^{T}$ and $p_{k+1}$ and $p_{k}$ are orthogonal.  \n",
    "So if $p_{k}$ is in the direction of $(-1, -1)^{T}$  \n",
    "We will have:\n",
    "$$\\begin{array}{l}\n",
    "p_{k}^{T}p_{k+1} &= (-1, -1)^{T} (\\lambda_{1}x_{1}^{(k+1)}, \\lambda_{2}x_{2}^{(k+1)})\\\\\n",
    "&= 0\n",
    "\\end{array}$$\n",
    "Which means $ \\lambda_{1}x_{1}^{(k+1)} = - \\lambda_{2}x_{2}^{(k+1)})$  \n",
    "Therefore $p_{k+1} = c(-1, 1)^{T}$, where $c$ is a constant factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p_{k} = \\nabla V(\\bar{x}) = (\\lambda_{1}x_{1}, \\lambda_{2}x_{2})^{T}$.  \n",
    "\n",
    "So if $p_{k}$ is in the direction of $(-1, -1)^{T}$, we will have:  \n",
    "$(\\lambda_{1}x_{1}, \\lambda_{2}x_{2})^{T} = c(-1, -1)^{T}$\n",
    "\n",
    "Which means $(x_{1}, x_{2}) = r(\\lambda_{1}, \\lambda_{2})$, where $ r(\\lambda_{1}, \\lambda_{2}) = c(\\frac{-1}{\\lambda_{1}}, \\frac{-1}{\\lambda_{2}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without loss of generality, we start from $x^{(0)} = (\\lambda_{2}, \\lambda_{1})^{T}$\n",
    "\n",
    "The gradient at $x^{(k)}$ is $(\\lambda_{1}x_{1}, \\lambda_{2}x_{2})^{T}$, so we get  \n",
    "\n",
    "$x^{(k)} + t\\nabla V(x^{(k)}) = \\left[\\begin{array} \\\\ (1+t\\lambda_{1})x_{1}^{(k)} \\\\ (1+t\\lambda_{2})x_{2}^{(k)}  \\end{array} \\right] = \\left( \\frac{\\lambda_{1}-\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}} \\right)^{k} \\left[\\begin{array} \\\\ (-1)^{k}(\\lambda_{2}+t) \\\\ (\\lambda_{1}+t)  \\end{array} \\right]$\n",
    "and\n",
    "\n",
    "$V(x^{(k+1)}) = \\frac{1}{2}(\\lambda_{1}(t+\\lambda_{2})^{2} + \\lambda_{2}(t+\\lambda_{1})^{2}\\left( \\frac{\\lambda_{1}-\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}} \\right)^{2k}$\n",
    "\n",
    "Which is minimized by $t = -\\frac{2\\lambda_{1}\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}}$\n",
    "\n",
    "Therefore $x^{(k)} = \\left( \\frac{\\lambda_{1}-\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}} \\right)^{k} \\left[\\begin{array} \\\\ (-1)^{k}\\lambda_{2} \\\\ \\lambda_{1} \\end{array} \\right]$\n",
    "\n",
    "Hence \n",
    "\n",
    "$\\rho =  \\frac{\\Vert x^{(k+1)}\\Vert}{\\Vert x^{(k)}\\Vert} = \\frac{\\lambda_{1}-\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\lambda_{1} >> \\lambda_{2}$, $\\kappa(H) = \\max_{ x \\ne 0} \\frac{\\Vert Hx\\Vert}{\\Vert x \\Vert} = \\frac{\\lambda_{1}}{\\lambda_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore   \n",
    "\n",
    "$\\rho = \\frac{\\lambda_{1}-\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}} = \\frac{\\lambda_{1} + \\lambda_{2} - 2\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}} = 1-2\\frac{\\lambda_{2}}{\\lambda_{1}+\\lambda_{2}}  \\approx 1-2\\frac{\\lambda_{2}}{\\lambda_{1}} = 1-\\frac{2}{\\kappa(H)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\left(1-\\frac{2}{\\kappa(H)}\\right)^{n} e_{0} = e^{-2}$  \n",
    "\n",
    "We have $\\left(1-\\frac{2}{\\kappa(H)}\\right) = e^{-\\frac{2}{n}} \\approx 1 - \\frac{2}{n}$  \n",
    "\n",
    "Which indicates $n \\approx \\kappa(H)$  to reduce the error to $e^{-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "For the quadratic programming problem derived for portfolio optimization\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "\\\\\n",
    "\\min_{\\bf x} & &  \\frac{1}{2} \\lambda\\; {\\bf x}^T \\Sigma {\\bf x} - \\mu^T {\\bf x} \n",
    "\\\\\n",
    "s.t. & & \\Sigma x_i = 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the risk-aversion coefficient, $\\mu$ is the expected asset return vector and $\\Sigma$ is the covariance matrix.  Derive the dual problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand{ml}{\\mathbb{\\mathcal L}}\n",
    "\\renewcommand{bmu}{\\boldsymbol{ \\mu}}\n",
    "\\renewcommand{bx}{\\boldsymbol x}\n",
    "\\renewcommand{bs}{\\boldsymbol}\n",
    "\\renewcommand{bld}{\\boldsymbol{ \\lambda}}\n",
    "\\renewcommand{mD}{\\mathbb{\\mathcal D}}\n",
    "\\renewcommand{df}{\\hat{f}}\n",
    "\\renewcommand{ba}{\\boldsymbol \\alpha}\n",
    "$$\n",
    "Let $\\bs{g}(\\bx) = \\bs{1}^{T} \\bx - 1 $  \n",
    "\n",
    "Then\n",
    "$$\n",
    "\\ml(\\bx, \\ba) = f(\\bx) + \\ba \\bs{g}(\\bx)\\\\\n",
    "\\df(\\bld) = \\inf_{\\bx\\in \\mD}\\; \\ml(\\bx, \\ba) =\\inf_{\\bx\\in\\mD} \\left( \\frac{1}{2} \\lambda\\; {\\bf x}^T \\Sigma {\\bf x} - \\mu^T {\\bf x} + \\ba(\\bs{1}^{T} \\bx - 1) \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
